{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 4727518,
          "sourceType": "datasetVersion",
          "datasetId": 2735624
        }
      ],
      "dockerImageVersionId": 30407,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebook820df4ce94",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jacotaco_danish_golf_courses_orthophotos_path = kagglehub.dataset_download('jacotaco/danish-golf-courses-orthophotos')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "5PONOkt9LdTr"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.strategies.ddp import DDPStrategy\n",
        "\n",
        "import torch\n",
        "import torchmetrics\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image, ImageReadMode\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "#Plotting images\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-05-02T14:25:44.735768Z",
          "iopub.execute_input": "2023-05-02T14:25:44.736374Z",
          "iopub.status.idle": "2023-05-02T14:25:59.252118Z",
          "shell.execute_reply.started": "2023-05-02T14:25:44.736329Z",
          "shell.execute_reply": "2023-05-02T14:25:59.25106Z"
        },
        "trusted": true,
        "id": "ikbIqkZULdTv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters.\n",
        "\n",
        "Here we setup some hyperparameters for this notebook."
      ],
      "metadata": {
        "id": "yL2I8asvLdTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "BATCH_SIZE = 16 #Number of batches when training\n",
        "IMAGE_SIZE = (256, 256)#(320, 192) #Images get resized to a smaller resolution\n",
        "IN_CHANNELS = 3 #There are 3 channels for RGB\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "IMAGES_DIR = '/kaggle/input/danish-golf-courses-orthophotos/1. orthophotos/'\n",
        "SEGMASKS_DIR = '/kaggle/input/danish-golf-courses-orthophotos/2. segmentation masks/'\n",
        "LABELMASKS_DIR = '/kaggle/input/danish-golf-courses-orthophotos/3. class masks/'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:25:59.2556Z",
          "iopub.execute_input": "2023-05-02T14:25:59.256386Z",
          "iopub.status.idle": "2023-05-02T14:25:59.263171Z",
          "shell.execute_reply.started": "2023-05-02T14:25:59.256333Z",
          "shell.execute_reply": "2023-05-02T14:25:59.261688Z"
        },
        "trusted": true,
        "id": "U8cg3qt_LdTx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of the dataset.\n",
        "\n",
        "The dataset contains orthophotos of golf holes. Orthophotos are aerial images where the perpective has been geometrically corrected to a top-down view.\n",
        "\n",
        "Each orthophoto is named to contain the name of the golf course, the scale of the aerial image and the number of the image.\n",
        "I.e **'Benniksgaard_Golf_Klub_1000_01.jpg'** is the 1st orthophoto taken from the Benniksgaard golf club, and has a scale of 1:1000.\n",
        "Each orthophoto has a corresponding segmentation mask and a label mask. For example, the orthophoto **'Benniksgaard_Golf_Klub_1000_01.jpg'** has a corresponding segmentation mask and label mask named **'Benniksgaard_Golf_Klub_1000_001.png'** in their respective folders. Keep in mind the masks are png and not jpg like the orthophoto.\n",
        "\n",
        "The segmentation masks is the output from the CVAT annotation software in RGB values, and is mostly used in this notebook for visualizing the data. For training you should transform these 3 channel RGB values to a single channel label values. For example, every pixel that is water will have an RGB value of (0, 0, 255), and should be transformed a single value storing the label, e.g 5.\n",
        "We have made this process easier by already converting every picuture to a label mask, where each pixel's value corresponds to a class (0-5). This can be seen in the directory **'danish-golf-courses-orthophotos/3. class masks'**.\n",
        "\n",
        "The labels for each class is as follows:\n",
        "\n",
        "0. Background\n",
        "1. Fairway\n",
        "2. Green\n",
        "3. Tee\n",
        "4. Bunker\n",
        "5. Water\n",
        "\n"
      ],
      "metadata": {
        "id": "0vL_g1E_LdTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the data\n",
        "orthophoto_list = os.listdir(IMAGES_DIR)\n",
        "print(\"There are \", len(orthophoto_list), \" orthophotos in this dataset!\")\n",
        "\n",
        "#Load image with index of 5 (I prefer this image as it shows all the classes)\n",
        "idx = 5 #The index can be changed to view other orthophotos.\n",
        "golf_image = Image.open(os.path.join(IMAGES_DIR, orthophoto_list[idx]))\n",
        "golf_segmask = Image.open(os.path.join(SEGMASKS_DIR, orthophoto_list[idx].replace(\".jpg\", \".png\"))) #The class masks are png instead of jpg\n",
        "\n",
        "#Plot using matplotlib\n",
        "fig, axes = plt.subplots(1, 2)\n",
        "\n",
        "axes[0].set_title('Orthophoto')\n",
        "axes[1].set_title('Segmentation Mask')\n",
        "\n",
        "axes[0].imshow(golf_image)\n",
        "axes[1].imshow(golf_segmask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:25:59.264863Z",
          "iopub.execute_input": "2023-05-02T14:25:59.265244Z",
          "iopub.status.idle": "2023-05-02T14:26:00.361814Z",
          "shell.execute_reply.started": "2023-05-02T14:25:59.265207Z",
          "shell.execute_reply": "2023-05-02T14:26:00.36081Z"
        },
        "trusted": true,
        "id": "B65a5-CuLdTz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Dataset class\n",
        "\n",
        "We create a custom Dataset class to load our golf images.\n",
        "\n",
        "In Pytorch the Dataset class contain the functions: **\\__len__** and **\\__getitem__**, where **\\__len__** returns the amount of images in the dataset, and **\\__getitem__** returns the image and label for each image index.\n"
      ],
      "metadata": {
        "id": "OyA4IHlsLdTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GolfDataset(Dataset):\n",
        "    def __init__(self, images_dir, labelmasks_dir):\n",
        "        #The directories for each folder\n",
        "        self.images_dir = images_dir\n",
        "        self.labelmasks_dir = labelmasks_dir\n",
        "\n",
        "        self.images_dir_list = os.listdir(images_dir) #We create a list of PATHs to every file in the orthophotos directory.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_dir_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.images_dir, self.images_dir_list[idx])\n",
        "        image = read_image(image_path, ImageReadMode.RGB)\n",
        "\n",
        "        label_mask_path = os.path.join(self.labelmasks_dir, self.images_dir_list[idx]).replace(\".jpg\", \".png\") #The class masks are png instead of jpg\n",
        "        label_mask = read_image(label_mask_path, ImageReadMode.GRAY)\n",
        "\n",
        "        #Apply transformations to the images. This can be optimized using nn.Sequential or nn.Compose.\n",
        "        image = TF.resize(image, IMAGE_SIZE) #Apply resize transform\n",
        "        image = image.float()\n",
        "        image = image / 255 #Normalize values from [0-255] to [0-1]\n",
        "\n",
        "        label_mask = TF.resize(label_mask, IMAGE_SIZE) #Apply resize transform\n",
        "        label_mask = TF.rgb_to_grayscale(label_mask) #Apply grayscaling to go from 3->1 channels.\n",
        "        label_mask = label_mask.float()\n",
        "\n",
        "        return image, label_mask"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.364189Z",
          "iopub.execute_input": "2023-05-02T14:26:00.364575Z",
          "iopub.status.idle": "2023-05-02T14:26:00.375317Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.364533Z",
          "shell.execute_reply": "2023-05-02T14:26:00.373739Z"
        },
        "trusted": true,
        "id": "toANKWhsLdT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Dataset class\n",
        "\n",
        "We can quickly create a an instance of the GolfDataset class and print out values for an item in the dataset."
      ],
      "metadata": {
        "id": "RZFmZycRLdT0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "golf_ds = GolfDataset(IMAGES_DIR, LABELMASKS_DIR)\n",
        "idx = 5\n",
        "orthophoto = golf_ds.__getitem__(idx)[0]\n",
        "label_mask = golf_ds.__getitem__(idx)[1]\n",
        "print(\"Ortophoto: \", orthophoto.shape, orthophoto)\n",
        "print(\"Label:\", label_mask.shape, label_mask)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.376641Z",
          "iopub.execute_input": "2023-05-02T14:26:00.378162Z",
          "iopub.status.idle": "2023-05-02T14:26:00.555229Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.378122Z",
          "shell.execute_reply": "2023-05-02T14:26:00.553406Z"
        },
        "trusted": true,
        "id": "GPBwzo9gLdT0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the orthophoto has the shape **[3, 256, 256]**, which corresponds to **[channels, height, width]**. It has 3 channels due to being an RGB image, and the pixel values are normalized from 0-1 instead of 0-255.\n",
        "The labelmask has the shape **[1, 256, 256]**, because it only has 1 grayscale channel for the labels, which are values from **(0-5)**."
      ],
      "metadata": {
        "id": "UcJiW3tDLdT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a DataModule class\n",
        "\n",
        "Pytorch also has a DataModule class that loads the data from the Dataset class we just made. In this class we split the dataset into training, validation and testing with a **70/20/10** split. This DataModule class also allows us to set the batch size, number of workers and more for the training. Since we are using Pytorch lightning, we need to have the following functions: **prepare_data, setup, train_dataloader, val_dataloader and test_dataloader.**"
      ],
      "metadata": {
        "id": "Wjr-oCSDLdT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GolfDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.all_images = []\n",
        "\n",
        "    def prepare_data(self):\n",
        "        #We don't use this function for loading the data as prepare_data is called from a single GPU.\n",
        "        #It can also not be usedto assign state (self.x = y).\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        #Data is loaded from the image and mask directories\n",
        "        self.all_images = GolfDataset(IMAGES_DIR, LABELMASKS_DIR)\n",
        "        #The data is split into train, val and test with a 70/20/10 split\n",
        "        self.train_data, self.val_data, self.test_data = random_split(self.all_images, [0.7,0.2,0.1])\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_data, batch_size=self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "         return DataLoader(self.val_data, batch_size=self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "         return DataLoader(self.test_data, batch_size=self.batch_size, num_workers=2, pin_memory=True, persistent_workers=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.557073Z",
          "iopub.execute_input": "2023-05-02T14:26:00.557453Z",
          "iopub.status.idle": "2023-05-02T14:26:00.566437Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.557414Z",
          "shell.execute_reply": "2023-05-02T14:26:00.565203Z"
        },
        "trusted": true,
        "id": "Ue677j2LLdT1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the U-Net Model\n",
        "\n",
        "Here we create the **U-Net model** in Pytorch lightning. The decoder and encoder parts are created in the init function, and the forward function calls each layer. This class also contains functions for each training and validation step, where we predict an image using the U-Net model, then calculate the loss and return it to improve the model.\n",
        "\n",
        "Furthermore, we created a **save_predictions_as_images** function to save the images throughout training to the **/kaggle/working/** directory. This is for our own sake so we can visualize the training process while it is training. However in Kaggle we have to download these images individually. One of the next cells allows us to visualize this data using matplotlib, you can perform this while training or after training is complete."
      ],
      "metadata": {
        "id": "fSnUMy-zLdT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNetModel(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        #DoubleConvSame has padding=1 which keeps the input and ouput dimensions the same.\n",
        "        class DoubleConvSame(nn.Module):\n",
        "            def __init__(self, c_in, c_out):\n",
        "                super(DoubleConvSame, self).__init__()\n",
        "                self.conv = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels=c_in, out_channels=c_out, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(in_channels=c_out, out_channels=c_out, kernel_size=3, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                )\n",
        "\n",
        "            def forward(self, x):\n",
        "                return self.conv(x)\n",
        "\n",
        "        self.conv1 = DoubleConvSame(c_in=3, c_out=64)\n",
        "        self.conv2 = DoubleConvSame(c_in=64, c_out=128)\n",
        "        self.conv3 = DoubleConvSame(c_in=128, c_out=256)\n",
        "        self.conv4 = DoubleConvSame(c_in=256, c_out=512)\n",
        "        self.conv5 = DoubleConvSame(c_in=512, c_out=1024)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.up1 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2)\n",
        "        self.up2 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2)\n",
        "        self.up3 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2)\n",
        "        self.up4 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2)\n",
        "\n",
        "        self.up_conv1 = DoubleConvSame(c_in=1024, c_out=512)\n",
        "        self.up_conv2 = DoubleConvSame(c_in=512, c_out=256)\n",
        "        self.up_conv3 = DoubleConvSame(c_in=256, c_out=128)\n",
        "        self.up_conv4 = DoubleConvSame(c_in=128, c_out=64)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(in_channels=64, out_channels=6, kernel_size=1)\n",
        "\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "\n",
        "    def crop_tensor(self, up_tensor, target_tensor):\n",
        "        _, _, H, W = up_tensor.shape\n",
        "\n",
        "        x = T.CenterCrop(size=(H, W))(target_tensor)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"ENCODER\"\"\"\n",
        "\n",
        "        c1 = self.conv1(x)\n",
        "        p1 = self.pool(c1)\n",
        "\n",
        "\n",
        "        c2 = self.conv2(p1)\n",
        "        p2 = self.pool(c2)\n",
        "\n",
        "\n",
        "        c3 = self.conv3(p2)\n",
        "        p3 = self.pool(c3)\n",
        "\n",
        "        c4 = self.conv4(p3)\n",
        "        p4 = self.pool(c4)\n",
        "        \"\"\"BOTTLE-NECK\"\"\"\n",
        "\n",
        "        c5 = self.conv5(p4)\n",
        "        \"\"\"DECODER\"\"\"\n",
        "\n",
        "        u1 = self.up1(c5)\n",
        "        crop1 = self.crop_tensor(u1, c4)\n",
        "        cat1 = torch.cat([u1, crop1], dim=1)\n",
        "        uc1 = self.up_conv1(cat1)\n",
        "\n",
        "        u2 = self.up2(uc1)\n",
        "        crop2 = self.crop_tensor(u2, c3)\n",
        "        cat2 = torch.cat([u2, crop2], dim=1)\n",
        "        uc2 = self.up_conv2(cat2)\n",
        "\n",
        "        u3 = self.up3(uc2)\n",
        "        crop3 = self.crop_tensor(u3, c2)\n",
        "        cat3 = torch.cat([u3, crop3], dim=1)\n",
        "        uc3 = self.up_conv3(cat3)\n",
        "\n",
        "        u4 = self.up4(uc3)\n",
        "        crop4 = self.crop_tensor(u4, c1)\n",
        "        cat4 = torch.cat([u4, crop4], dim=1)\n",
        "        uc4 = self.up_conv4(cat4)\n",
        "\n",
        "        outputs = self.conv_1x1(uc4)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.forward(x)\n",
        "        _y = torch.squeeze(y).long() #Squeeze to go from (B, 1, H, W) to (B, H, W), and converted to dtype of long - Needed for cross entropy loss!\n",
        "\n",
        "        loss = self.loss_fn(y_pred, _y)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "\n",
        "        y_pred = self.forward(x)\n",
        "        _y = torch.squeeze(y).long() #Squeeze to go from (B, 1, H, W) to (B, H, W), and converted to dtype of long - Needed for cross entropy loss!\n",
        "\n",
        "        loss = self.loss_fn(y_pred, _y)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_pred = self.forward(x)\n",
        "        _y = torch.squeeze(y).long() #Squeeze to go from (B, 1, H, W) to (B, H, W), and converted to dtype of long - Needed for cross entropy loss!\n",
        "\n",
        "        loss = self.loss_fn(y_pred, _y)\n",
        "\n",
        "        save_predictions_as_imgs(x, y, y_pred, counter=batch_idx)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def test_epoch_end(self, outs):\n",
        "        print(\"Testing ended!\")\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.AdamW(self.parameters(), lr=LEARNING_RATE)\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.568541Z",
          "iopub.execute_input": "2023-05-02T14:26:00.569322Z",
          "iopub.status.idle": "2023-05-02T14:26:00.595028Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.569214Z",
          "shell.execute_reply": "2023-05-02T14:26:00.593822Z"
        },
        "trusted": true,
        "id": "Uf606RegLdT1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a function for saving the predictions as png files"
      ],
      "metadata": {
        "id": "ZaAluYCdLdT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder=\"/kaggle/working/\"\n",
        "def save_predictions_as_imgs(x, y, y_pred, counter=0):\n",
        "    #Currently the groundtruth and prediction (y & y_pred) have the shape [B, C, H, W] = [B, 1, H, W].\n",
        "    #If we save them as images it will be in grayscale as the number of channels is 1.\n",
        "    #Therefor, we have to convert them to 3 channels (RGB), and each class gets their own color.\n",
        "    y_in_rgb = torch.zeros(y.shape[0], 3, y.shape[2], y.shape[3]).to(y.device)\n",
        "\n",
        "    y_pred_in_rgb = torch.zeros(y.shape[0], 3, y.shape[2], y.shape[3]).to(y.device)\n",
        "    #Create a list of tensors containing the rgb colors for each class\n",
        "    #The list is [Background, Fairway, Green, Tee, Bunker, Water]\n",
        "    class_colors = [torch.tensor([0, 0, 0]).to(y.device), torch.tensor([0.0, 140.0/255, 0.0]).to(y.device), torch.tensor([0.0, 1.0, 0.0]).to(y.device), torch.tensor([1.0, 0.0, 0.0]).to(y.device), torch.tensor([217.0/255, 230.0/255, 122.0/255]).to(y.device), torch.tensor([7.0/255, 15.0/255, 247.0/255]).to(y.device)]\n",
        "\n",
        "    y_pred = calculate_labels_from_pred(y_pred) #Converted the prediction (stored as probalities) to labels!\n",
        "    for c in range(1, 6): #loop through the classes 1-5\n",
        "        y_mask = torch.where(y == c, 1, 0).to(y.device)\n",
        "        y_pred_mask = torch.where(y_pred == c, 1, 0).to(y_pred.device)\n",
        "        current_class_color = class_colors[c].reshape(1, 3, 1, 1)\n",
        "        y_segment = y_mask*current_class_color\n",
        "        y_pred_segment = y_pred_mask*current_class_color\n",
        "        y_in_rgb += y_segment\n",
        "        y_pred_in_rgb += y_pred_segment\n",
        "\n",
        "    #Save images to /kaggle/working/\n",
        "    #The images can be downloaded, or visualized later with matplotlib!\n",
        "    torchvision.utils.save_image(x, f\"{folder}/{counter+1}_figure.jpg\")\n",
        "    torchvision.utils.save_image(y_in_rgb, f\"{folder}/{counter+1}_groundtruth.jpg\")\n",
        "    torchvision.utils.save_image(y_pred_in_rgb, f\"{folder}/{counter+1}_prediction.jpg\")\n",
        "\n",
        "softmax = nn.Softmax2d()\n",
        "def calculate_labels_from_pred(pred):\n",
        "    pred = softmax(pred)\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    pred = pred.float()\n",
        "    pred = pred.unsqueeze(1)\n",
        "    pred.requires_grad_()\n",
        "    return pred"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.596515Z",
          "iopub.execute_input": "2023-05-02T14:26:00.59719Z",
          "iopub.status.idle": "2023-05-02T14:26:00.611226Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.597154Z",
          "shell.execute_reply": "2023-05-02T14:26:00.610115Z"
        },
        "trusted": true,
        "id": "FdnlY_s0LdT2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training our model\n",
        "\n",
        "This cell trains our model in pytorch lightning.\n",
        "\n",
        "The **GolfDataModule** class loads our data, and the **U-Net** class loads our model.\n",
        "The **trainer** creates an instance of the trainer class with specified parameters such as the number of epochs, accelerators and devices. For this notebook we have used the **T4 x2 GPU** that kaggle provides us. If you are using something else, then change the parameters to fit your setup."
      ],
      "metadata": {
        "id": "BG2DoqoJLdT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = GolfDataModule(BATCH_SIZE)\n",
        "trainer = pl.Trainer(max_epochs=50, accelerator='gpu', devices=2, log_every_n_steps=24, strategy=\"ddp_notebook_find_unused_parameters_false\")\n",
        "model = UNetModel()\n",
        "\n",
        "\n",
        "trainer.fit(model, train_loader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:26:00.614013Z",
          "iopub.execute_input": "2023-05-02T14:26:00.614473Z",
          "iopub.status.idle": "2023-05-02T14:28:37.991301Z",
          "shell.execute_reply.started": "2023-05-02T14:26:00.614424Z",
          "shell.execute_reply": "2023-05-02T14:28:37.989792Z"
        },
        "trusted": true,
        "id": "XDI60ta_LdT3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Kaggle's GPU T4 x 2, it takes around 30-60 minutes to train the model. The max_epochs parameters can be decreased to train faster.\n",
        "\n",
        "The model can now be tested. Pytorch Lightning automatically saves the best model for us."
      ],
      "metadata": {
        "id": "vrjvbrY1LdT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# automatically loads the best weights for you\n",
        "trainer = pl.Trainer(devices=1, num_nodes=1, accelerator='gpu')\n",
        "trainer.test(model, train_loader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:28:37.995211Z",
          "iopub.execute_input": "2023-05-02T14:28:37.995919Z",
          "iopub.status.idle": "2023-05-02T14:28:47.45023Z",
          "shell.execute_reply.started": "2023-05-02T14:28:37.995851Z",
          "shell.execute_reply": "2023-05-02T14:28:47.449033Z"
        },
        "trusted": true,
        "id": "zAaT44jpLdT3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the ouput\n",
        "\n",
        "Now that the model has been trained, we can visualize the output from the most recent validation images!"
      ],
      "metadata": {
        "id": "pUccrIDLLdT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/kaggle/working/'\n",
        "\n",
        "#Load the latest images from the validation!\n",
        "for idx in range(1, 8): #Show some of the batches\n",
        "    orthophoto = Image.open(output_dir + str(idx) + '_figure.jpg')\n",
        "    groundtruth = Image.open(output_dir + str(idx) + '_groundtruth.jpg')\n",
        "    prediction = Image.open(output_dir + str(idx) + '_prediction.jpg')\n",
        "\n",
        "    #Plot using matplotlib\n",
        "    fig, axes = plt.subplots(1, 3)\n",
        "    fig.set_size_inches(18.5, 15.5)\n",
        "\n",
        "    axes[0].set_title('Orthophoto')\n",
        "    axes[1].set_title('Groundtruth')\n",
        "    axes[2].set_title('Prediction')\n",
        "\n",
        "    axes[0].imshow(orthophoto)\n",
        "    axes[1].imshow(groundtruth)\n",
        "    axes[2].imshow(prediction)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-05-02T14:28:47.452206Z",
          "iopub.execute_input": "2023-05-02T14:28:47.452594Z",
          "iopub.status.idle": "2023-05-02T14:28:54.760786Z",
          "shell.execute_reply.started": "2023-05-02T14:28:47.452542Z",
          "shell.execute_reply": "2023-05-02T14:28:54.759866Z"
        },
        "trusted": true,
        "id": "mIkg61NgLdT3"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}