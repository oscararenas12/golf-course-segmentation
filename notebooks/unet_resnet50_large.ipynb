{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# U-Net with ResNet50 Encoder for Golf Course Segmentation\n",
        "\n",
        "## Architecture Overview\n",
        "\n",
        "This notebook implements a **U-Net** architecture with a **ResNet50 encoder** for semantic segmentation of golf course aerial imagery.\n",
        "\n",
        "### Why U-Net?\n",
        "- **Encoder-Decoder structure**: Captures both high-level semantic features and low-level spatial details\n",
        "- **Skip connections**: Preserve fine-grained spatial information lost during downsampling\n",
        "- **Proven for segmentation**: Originally designed for biomedical image segmentation, works well for aerial imagery\n",
        "\n",
        "### Why ResNet50 Encoder?\n",
        "- **Transfer learning**: ImageNet pretrained weights provide robust feature extraction\n",
        "- **Residual connections**: Enable training of deeper networks without degradation\n",
        "- **Multi-scale features**: Different ResNet blocks capture features at 1/2, 1/4, 1/8, 1/16, 1/32 resolution\n",
        "\n",
        "### Segmentation Classes (6)\n",
        "| Class | Color | Description |\n",
        "|-------|-------|-------------|\n",
        "| 0 | Black | Background |\n",
        "| 1 | Dark Green | Fairway |\n",
        "| 2 | Bright Green | Green (putting surface) |\n",
        "| 3 | Red | Tee box |\n",
        "| 4 | Sandy Yellow | Bunker |\n",
        "| 5 | Blue | Water hazard |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment detection for Colab/local compatibility\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Running on Google Colab\")\n",
        "    !pip install -q kagglehub\n",
        "else:\n",
        "    print(\"Running locally\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GPU Configuration\n",
        "\n",
        "**Mixed Precision Training (float16)**:\n",
        "- Reduces memory usage by ~50%, enabling larger batch sizes\n",
        "- Speeds up computation on modern GPUs with Tensor Cores\n",
        "- Maintains float32 for numerically sensitive operations (loss, gradients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    # Memory growth prevents TF from allocating all GPU memory at once\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    # Enable mixed precision for faster training\n",
        "    keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "    print(f\"GPU configured: {len(gpus)} device(s)\")\n",
        "    for gpu in gpus:\n",
        "        print(f\"  - {gpu}\")\n",
        "else:\n",
        "    print(\"No GPU detected, using CPU\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download Danish Golf Courses dataset from Kaggle\n",
        "# First run may require Kaggle authentication (upload kaggle.json)\n",
        "dataset_path = kagglehub.dataset_download('jacotaco/danish-golf-courses-orthophotos')\n",
        "print(f\"Dataset path: {dataset_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters\n",
        "\n",
        "| Parameter | Value | Rationale |\n",
        "|-----------|-------|----------|\n",
        "| Image Size | 512×832 | Maintains aspect ratio of orthophotos, fits in GPU memory |\n",
        "| Batch Size | 2 | Limited by GPU memory due to large image size |\n",
        "| Learning Rate | 1e-4 | Conservative for fine-tuning pretrained encoder |\n",
        "| Augmentation | 25% | Prevents overfitting without excessive distortion |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 2          # Reduce to 1 if OOM on Colab T4\n",
        "IMAGE_SIZE = (512, 832) # Height × Width - maintains orthophoto aspect ratio\n",
        "IN_CHANNELS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "NUM_CLASSES = 6\n",
        "MAX_EPOCHS = 10\n",
        "AUGMENTATION_PROBABILITY = 0.25\n",
        "\n",
        "# Dataset structure:\n",
        "# - orthophotos/: RGB satellite images (.jpg)\n",
        "# - segmentation masks/: Color-coded visualization (.png)\n",
        "# - class masks/: Integer class labels 0-5 (.png)\n",
        "base_path = dataset_path\n",
        "IMAGES_DIR = os.path.join(base_path, '1. orthophotos')\n",
        "SEGMASKS_DIR = os.path.join(base_path, '2. segmentation masks')\n",
        "LABELMASKS_DIR = os.path.join(base_path, '3. class masks')\n",
        "\n",
        "OUTPUT_DIR = '/content/output' if IN_COLAB else './output'\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview a sample image and its segmentation mask\n",
        "orthophoto_list = os.listdir(IMAGES_DIR)\n",
        "print(f\"Total images: {len(orthophoto_list)}\")\n",
        "\n",
        "idx = 5\n",
        "golf_image = Image.open(os.path.join(IMAGES_DIR, orthophoto_list[idx]))\n",
        "golf_segmask = Image.open(os.path.join(SEGMASKS_DIR, orthophoto_list[idx].replace(\".jpg\", \".png\")))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].set_title('Orthophoto')\n",
        "axes[1].set_title('Segmentation Mask')\n",
        "axes[0].imshow(golf_image)\n",
        "axes[1].imshow(golf_segmask)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Pipeline\n",
        "\n",
        "### Preprocessing\n",
        "- **Images**: Resize to 512×832, normalize to [0, 1]\n",
        "- **Masks**: Resize with nearest-neighbor interpolation (preserves integer class labels)\n",
        "\n",
        "### Synchronized Augmentation\n",
        "For segmentation, image and mask must be augmented identically:\n",
        "1. Concatenate image (3 channels) and mask (1 channel) → 4-channel tensor\n",
        "2. Apply geometric transforms (flip, rotation) to combined tensor\n",
        "3. Split back into image and mask\n",
        "4. Apply photometric transforms (brightness, contrast) to image only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path, mask_path):\n",
        "    \"\"\"Load image-mask pair with proper preprocessing.\"\"\"\n",
        "    # Load and normalize image to [0, 1]\n",
        "    image = tf.io.read_file(image_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    # Load mask - use nearest neighbor to preserve class labels\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "    mask = tf.image.resize(mask, IMAGE_SIZE, method='nearest')\n",
        "    mask = tf.cast(mask, tf.float32)\n",
        "    mask = tf.squeeze(mask, axis=-1)  # Remove channel dimension\n",
        "\n",
        "    return image, mask\n",
        "\n",
        "\n",
        "def augment_image_and_mask(image, mask):\n",
        "    \"\"\"Synchronized augmentation - same transform applied to both.\"\"\"\n",
        "    def apply_augmentation():\n",
        "        # Concatenate for synchronized geometric transforms\n",
        "        mask_expanded = tf.expand_dims(mask, axis=-1)\n",
        "        combined = tf.concat([image, mask_expanded], axis=-1)\n",
        "        \n",
        "        # Geometric: random horizontal flip\n",
        "        combined = tf.image.random_flip_left_right(combined)\n",
        "        \n",
        "        # Split back\n",
        "        aug_image = combined[:, :, :3]\n",
        "        aug_mask = combined[:, :, 3]\n",
        "        \n",
        "        # Photometric: only apply to image (not mask)\n",
        "        aug_image = tf.image.random_brightness(aug_image, 0.1)\n",
        "        aug_image = tf.image.random_contrast(aug_image, 0.9, 1.1)\n",
        "        aug_image = tf.clip_by_value(aug_image, 0.0, 1.0)\n",
        "        \n",
        "        return tf.cast(aug_image, tf.float32), aug_mask\n",
        "\n",
        "    def keep_original():\n",
        "        return tf.cast(image, tf.float32), mask\n",
        "\n",
        "    # Stochastic augmentation: only augment 25% of samples\n",
        "    should_augment = tf.random.uniform([]) < AUGMENTATION_PROBABILITY\n",
        "    return tf.cond(should_augment, apply_augmentation, keep_original)\n",
        "\n",
        "\n",
        "def create_dataset(images_dir, labelmasks_dir, shuffle=True):\n",
        "    \"\"\"Create tf.data.Dataset from directory.\"\"\"\n",
        "    image_filenames = sorted(os.listdir(images_dir))\n",
        "    image_paths = [os.path.join(images_dir, fname) for fname in image_filenames]\n",
        "    mask_paths = [os.path.join(labelmasks_dir, fname.replace('.jpg', '.png')) for fname in image_filenames]\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, mask_paths))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(image_paths), seed=42)\n",
        "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset, len(image_paths)\n",
        "\n",
        "\n",
        "def prepare_datasets():\n",
        "    \"\"\"70/20/10 train/val/test split.\"\"\"\n",
        "    full_dataset, total_size = create_dataset(IMAGES_DIR, LABELMASKS_DIR, shuffle=True)\n",
        "\n",
        "    train_size = int(0.7 * total_size)\n",
        "    val_size = int(0.2 * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    print(f\"Split: {train_size} train, {val_size} val, {test_size} test\")\n",
        "\n",
        "    train_ds = full_dataset.take(train_size)\n",
        "    remaining = full_dataset.skip(train_size)\n",
        "    val_ds = remaining.take(val_size)\n",
        "    test_ds = remaining.skip(val_size)\n",
        "\n",
        "    # Only augment training data\n",
        "    train_ds = train_ds.map(augment_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    train_ds = train_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    return train_ds, val_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## U-Net Architecture\n",
        "\n",
        "```\n",
        "Input (512×832×3)\n",
        "    │\n",
        "    ▼\n",
        "┌─────────────────────────────────────────┐\n",
        "│         ResNet50 ENCODER                │\n",
        "│  (ImageNet pretrained, frozen)          │\n",
        "│                                         │\n",
        "│  conv1_relu ──────────────────────────┐ │  1/2 (256×416)\n",
        "│       │                               │ │\n",
        "│  conv2_block3 ───────────────────┐    │ │  1/4 (128×208)\n",
        "│       │                          │    │ │\n",
        "│  conv3_block4 ──────────────┐    │    │ │  1/8 (64×104)\n",
        "│       │                     │    │    │ │\n",
        "│  conv4_block6 ─────────┐    │    │    │ │  1/16 (32×52)\n",
        "│       │                │    │    │    │ │\n",
        "│  conv5_block3 ───┐     │    │    │    │ │  1/32 (16×26) BOTTLENECK\n",
        "└──────────────────│─────│────│────│────│─┘\n",
        "                   │     │    │    │    │\n",
        "┌──────────────────│─────│────│────│────│─┐\n",
        "│         DECODER  │     │    │    │    │ │\n",
        "│                  │     │    │    │    │ │\n",
        "│  UpConv 512 ─────┘     │    │    │    │ │\n",
        "│  + skip ───────────────┘    │    │    │ │  1/16\n",
        "│  Conv 512×2                 │    │    │ │\n",
        "│       │                     │    │    │ │\n",
        "│  UpConv 256                 │    │    │ │\n",
        "│  + skip ────────────────────┘    │    │ │  1/8\n",
        "│  Conv 256×2                      │    │ │\n",
        "│       │                          │    │ │\n",
        "│  UpConv 128                      │    │ │\n",
        "│  + skip ─────────────────────────┘    │ │  1/4\n",
        "│  Conv 128×2                           │ │\n",
        "│       │                               │ │\n",
        "│  UpConv 64                            │ │\n",
        "│  + skip ──────────────────────────────┘ │  1/2\n",
        "│  Conv 64×2                              │\n",
        "│       │                                 │\n",
        "│  UpConv 32                              │  Full resolution\n",
        "│  Conv 32×2                              │\n",
        "│       │                                 │\n",
        "│  Conv 1×1 (6 classes)                   │\n",
        "└─────────────────────────────────────────┘\n",
        "    │\n",
        "    ▼\n",
        "Output (512×832×6) logits\n",
        "```\n",
        "\n",
        "### Key Design Choices\n",
        "- **Skip connections**: Concatenate encoder features at each resolution to preserve spatial detail\n",
        "- **Conv2DTranspose**: Learned upsampling (better than bilinear interpolation)\n",
        "- **Double convolutions**: Two 3×3 convs at each decoder level for feature refinement\n",
        "- **1×1 output conv**: Maps features to class logits without spatial reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_unet_resnet50(input_shape=(512, 832, 3), num_classes=6):\n",
        "    \"\"\"U-Net with ResNet50 encoder pretrained on ImageNet.\"\"\"\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # ENCODER: ResNet50 backbone\n",
        "    # Using pretrained weights enables strong feature extraction\n",
        "    # even with limited training data\n",
        "    base_model = keras.applications.ResNet50(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs\n",
        "    )\n",
        "\n",
        "    # Extract skip connections at each resolution\n",
        "    # These preserve spatial details lost during downsampling\n",
        "    skip_layer_names = [\n",
        "        'conv1_relu',        # 1/2 resolution, 64 channels\n",
        "        'conv2_block3_out',  # 1/4 resolution, 256 channels\n",
        "        'conv3_block4_out',  # 1/8 resolution, 512 channels\n",
        "        'conv4_block6_out',  # 1/16 resolution, 1024 channels\n",
        "    ]\n",
        "    skip_connections = [base_model.get_layer(name).output for name in skip_layer_names]\n",
        "    bottleneck = base_model.get_layer('conv5_block3_out').output  # 1/32, 2048 channels\n",
        "\n",
        "    # DECODER: Progressive upsampling with skip connections\n",
        "    # Each decoder block: UpConv → Concat skip → Conv → Conv\n",
        "    \n",
        "    # 1/32 → 1/16\n",
        "    x = layers.Conv2DTranspose(512, kernel_size=2, strides=2, padding='same')(bottleneck)\n",
        "    x = layers.Concatenate()([x, skip_connections[3]])\n",
        "    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(512, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # 1/16 → 1/8\n",
        "    x = layers.Conv2DTranspose(256, kernel_size=2, strides=2, padding='same')(x)\n",
        "    x = layers.Concatenate()([x, skip_connections[2]])\n",
        "    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(256, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # 1/8 → 1/4\n",
        "    x = layers.Conv2DTranspose(128, kernel_size=2, strides=2, padding='same')(x)\n",
        "    x = layers.Concatenate()([x, skip_connections[1]])\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # 1/4 → 1/2\n",
        "    x = layers.Conv2DTranspose(64, kernel_size=2, strides=2, padding='same')(x)\n",
        "    x = layers.Concatenate()([x, skip_connections[0]])\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # 1/2 → full resolution\n",
        "    x = layers.Conv2DTranspose(32, kernel_size=2, strides=2, padding='same')(x)\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
        "    x = layers.Conv2D(32, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "    # Output: 1×1 conv to map to class logits\n",
        "    # dtype='float32' ensures numerical stability with mixed precision\n",
        "    outputs = layers.Conv2D(num_classes, kernel_size=1, padding='same', dtype='float32')(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=outputs, name='UNet_ResNet50')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "### Loss Function: Sparse Categorical Crossentropy\n",
        "- `from_logits=True`: Model outputs raw logits, softmax applied in loss\n",
        "- \"Sparse\": Masks contain integer class labels (0-5), not one-hot vectors\n",
        "\n",
        "### Optimizer: AdamW\n",
        "- Adam with decoupled weight decay\n",
        "- Better generalization than standard Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = build_unet_resnet50(input_shape=(*IMAGE_SIZE, 3), num_classes=NUM_CLASSES)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.AdamW(learning_rate=LEARNING_RATE),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds, val_ds, test_ds = prepare_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Callbacks\n",
        "\n",
        "- **ModelCheckpoint**: Save best model based on validation loss\n",
        "- **EarlyStopping**: Stop if validation loss doesn't improve for 10 epochs\n",
        "- **ReduceLROnPlateau**: Halve learning rate if plateau for 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callback_list = [\n",
        "    callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(OUTPUT_DIR, 'best_unet_resnet50.keras'),\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5,\n",
        "        verbose=1,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=MAX_EPOCHS,\n",
        "    callbacks=callback_list,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\n",
        "\n",
        "Convert integer class masks to RGB using the standard color scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class color mapping (RGB normalized to [0,1])\n",
        "CLASS_COLORS = np.array([\n",
        "    [0, 0, 0],        # 0: Background - Black\n",
        "    [0, 140, 0],      # 1: Fairway - Dark Green\n",
        "    [0, 255, 0],      # 2: Green - Bright Green\n",
        "    [255, 0, 0],      # 3: Tee - Red\n",
        "    [217, 230, 122],  # 4: Bunker - Sandy Yellow\n",
        "    [7, 15, 247]      # 5: Water - Blue\n",
        "], dtype=np.float32) / 255.0\n",
        "\n",
        "CLASS_NAMES = ['Background', 'Fairway', 'Green', 'Tee', 'Bunker', 'Water']\n",
        "\n",
        "\n",
        "def mask_to_rgb(mask):\n",
        "    \"\"\"Convert integer class mask to RGB visualization.\"\"\"\n",
        "    h, w = mask.shape\n",
        "    rgb_mask = np.zeros((h, w, 3), dtype=np.float32)\n",
        "    for class_id in range(NUM_CLASSES):\n",
        "        rgb_mask[mask == class_id] = CLASS_COLORS[class_id]\n",
        "    return rgb_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions on test set\n",
        "for images, masks in test_ds.take(3):\n",
        "    # Get predictions (logits) and convert to class indices\n",
        "    predictions = model.predict(images, verbose=0)\n",
        "    pred_masks = np.argmax(predictions, axis=-1)\n",
        "    \n",
        "    for i in range(min(2, images.shape[0])):\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "        \n",
        "        axes[0].imshow(images[i].numpy())\n",
        "        axes[0].set_title('Input')\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        axes[1].imshow(mask_to_rgb(masks[i].numpy().astype(np.int32)))\n",
        "        axes[1].set_title('Ground Truth')\n",
        "        axes[1].axis('off')\n",
        "        \n",
        "        axes[2].imshow(mask_to_rgb(pred_masks[i]))\n",
        "        axes[2].set_title('Prediction')\n",
        "        axes[2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(os.path.join(OUTPUT_DIR, 'final_unet_resnet50.keras'))\n",
        "print(f\"Model saved to {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Train')\n",
        "plt.plot(history.history['val_loss'], label='Val')\n",
        "plt.title('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Train')\n",
        "plt.plot(history.history['val_accuracy'], label='Val')\n",
        "plt.title('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download trained model (Colab only)\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download(os.path.join(OUTPUT_DIR, 'final_unet_resnet50.keras'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
